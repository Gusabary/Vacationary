# Learning notes of Week 3

## 8.23 Fri.

+ 全链接（Fully connect）：相邻层之间的神经元两两都有连接。

  前馈（Feedforward）：信号只从后往前传。

+ > 对于神经网络来说，我们采用交叉熵（cross entropy）函数来对损失进行计算，接下来我们就是调整参数，让交叉熵越小越好。

+ > 在神经网络中计算损失最好的方法就是反向传播，我们可以用很多框架来进行计算损失，比如说TensorFlow，theano，Pytorch等等。

+ > sigmoid更一般的来说是激活函数(activation function)，现在已经很少用sigmoid来当做激活函数。

+ 深度学习的三个步骤：

  + 模型：神经网络

  + 损失函数：交叉熵

  + 梯度下降：用反向传播计算损失函数（L）对参数（w，（我也不知道 b 哪里去了））的偏微分

    其中分为两步，第一步（Forward pass）计算 z 对 w 的偏微分，第二步（Backward pass）计算 L 对 z 的偏微分，最后由链式法则求得结果。

+ 不管是回归模型还是神经网络，计算梯度下降需要损失函数对参数的偏微分，微分结果是一个含 y(hat) , x, w, b 的函数（y(hat) 意思是实际值，打不出来那个符号，另外 y 的预测值被代换成 x 和参数），然后将训练数据集代入，就得到一个只含参数（w, b）的函数，即损失函数的梯度，将当前的参数值代入就能得到在某点的梯度值，然后据此调整参数。（其实我觉得更直观的方法是算出梯度函数之后直接求零点（指让损失函数取得最小时的零点），然后一步到位把参数直接调整到零点就行了，不过可能函数过于复杂，即便是对于计算机来说直接求零点也很麻烦，而代入计算显得就简单很多了）

+ 全链接通过 dense 指定

+ > optimizer后面可以跟不同的方式，这些方式都是梯度下降，只是用的learning rate不同

+ 每计算完一个 batch 的梯度下降，就更新一次参数，把所有的 batch 都遍历一遍叫做一个 epoch。

##### Last-modified date: 2019.8.23, 11 p.m.